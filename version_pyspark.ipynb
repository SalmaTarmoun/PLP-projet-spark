{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import findspark\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation de la session Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.executor.memory\", '8g').config(\"spark.driver.memory\",'8g').config(\"spark.python.worker.memory\", '8g').config(\"spark.executor.cores\", '2').config(\"spark.driver.maxResultSize\", '0').config(\"spark.sql.crossJoin.enabled\", \"true\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture des fichiers :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+------+\n",
      "|   _c0|              review|rating|\n",
      "+------+--------------------+------+\n",
      "|206461|\"\"\"It has no side...|   9.0|\n",
      "|138000|\"\"\"This is my fir...|   8.0|\n",
      "| 35696|\"\"\"Suboxone has c...|   9.0|\n",
      "|155963|\"\"\"2nd day on 5mg...|   2.0|\n",
      "|165907|\"\"\"He pulled out,...|   1.0|\n",
      "+------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train = spark.read.option(\"sep\", \"\\t\").option(\"header\", \"true\").option(\"inferschema\", \"true\").option(\"mode\", \"DROPMALFORMED\").csv('data\\drugsComTrain_raw.tsv')\n",
    "df_test = spark.read.option(\"sep\", \"\\t\").option(\"header\", \"true\").option(\"inferschema\", \"true\").option(\"mode\", \"DROPMALFORMED\").csv('data\\drugsComTest_raw.tsv')\n",
    "df_train = df_train.select(['_c0', 'review', 'rating'])\n",
    "df_test = df_test.select(['_c0', 'review', 'rating'])   \n",
    "df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+------+\n",
      "|   _c0|              review|rating|\n",
      "+------+--------------------+------+\n",
      "|163740|\"\"\"I&#039;ve trie...|  10.0|\n",
      "|206473|\"\"\"My son has Cro...|   8.0|\n",
      "|159672|\"\"\"Quick reductio...|   9.0|\n",
      "| 39293|\"\"\"Contrave combi...|   9.0|\n",
      "| 97768|\"\"\"I have been on...|   9.0|\n",
      "+------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit les fonctions de preprocessing et de transformation de la variable de sortie :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "# lower case everything\n",
    "def textLower(x):\n",
    "    return x.lower()\n",
    "\n",
    "#delete numbers and replace punctuation by space \n",
    "def keepletters(input_str):\n",
    "    input_str = re.sub(r'\\d+', '', input_str)\n",
    "    return input_str.translate(str.maketrans(string.punctuation,' '*32))\n",
    "\n",
    "#remove last space and first space\n",
    "def stripfc(input_str):\n",
    "    return input_str.strip()\n",
    "\n",
    "def tokenize_stpords(input_str):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(input_str)\n",
    "    return [i for i in tokens if not i in stop_words]\n",
    "\n",
    "def preprocess(x):\n",
    "    x = textLower(x)\n",
    "    x = keepletters(x)\n",
    "    x = stripfc(x)\n",
    "    tokens = tokenize_stpords(x)\n",
    "    tokens_stem = ' '.join([stemmer.stem(token) for token in tokens])\n",
    "    return tokens_stem\n",
    "\n",
    "def discretise(x):\n",
    "    if not isinstance(x, float):\n",
    "        try :\n",
    "            x = float(x)\n",
    "        except Exception:\n",
    "            return 0\n",
    "    if x <= 5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On effectue le traitement des données en utilisant la fonction preprocess et discretise :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing et catégorisation du training set prend 74.71499848365784, taille 143313\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "start = time()\n",
    "df_train_clean = df_train.rdd.map(lambda row: Row(row[0], preprocess(row[1]), discretise(row[2]))).toDF()\n",
    "df_train_clean = df_train_clean.withColumnRenamed('_1', 'ID').withColumnRenamed('_2', 'review').withColumnRenamed('_3', 'label')\n",
    "train_size = df_train_clean.count()\n",
    "end = time()\n",
    "print('Preprocessing et catégorisation du training set prend {}, taille {}'.format(end-start, train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing et catégorisation du test set prend 28.428353309631348, taille 47694\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "df_test_clean = df_test.rdd.map(lambda row: Row(row[0], preprocess(row[1]), discretise(row[2]))).toDF()\n",
    "df_test_clean = df_test_clean.withColumnRenamed('_1', 'ID').withColumnRenamed('_2', 'review').withColumnRenamed('_3', 'label')\n",
    "test_size = df_test_clean.count()\n",
    "end = time()\n",
    "print('Preprocessing et catégorisation du test set prend {}, taille {}'.format(end-start, test_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Etape de tokenisation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split des phrases en liste de mots prend 91.83711051940918, taille 191007\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "start = time()\n",
    "tokenizer = Tokenizer(inputCol='review', outputCol='review_words')\n",
    "df_train_words = tokenizer.transform(df_train_clean)\n",
    "df_test_words = tokenizer.transform(df_test_clean)\n",
    "train_size = df_train_words.count()\n",
    "test_size = df_test_words.count()\n",
    "end = time()\n",
    "print('Split des phrases en liste de mots prend {}, taille {}'.format(end-start, train_size+test_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vectorisation : calcul des vecteurs TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calcul du terme TF-IDF prend 161.70283460617065, taille 191007\n"
     ]
    }
   ],
   "source": [
    "# Hashing Term-Frequency\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "start = time()\n",
    "hashing_tf = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol='review_tf', numFeatures=10000)\n",
    "df_train_tf = hashing_tf.transform(df_train_words)\n",
    "df_test_tf = hashing_tf.transform(df_test_words)\n",
    "# Inverse Document Frequency\n",
    "idf = IDF(inputCol=hashing_tf.getOutputCol(), outputCol=\"features\")\n",
    "idf_model = idf.fit(df_train_tf) # fit to build the model on all the data, and then apply it line by line\n",
    "df_train_tfidf = idf_model.transform(df_train_tf)\n",
    "df_test_tfidf = idf_model.transform(df_test_tf)\n",
    "\n",
    "train_size = df_train_tfidf.count()\n",
    "test_size = df_test_tfidf.count()\n",
    "end = time()\n",
    "print('Calcul du terme TF-IDF prend {}, taille {}'.format(end-start, train_size+test_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase d'apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Régression logistique :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le training prend 84.52868366241455\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "start = time()\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "lrModel = lr.fit(df_train_tfidf)\n",
    "end = time()\n",
    "print('Le training prend {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation sur le test set prend 30.619154691696167\n",
      "Accuracy : 0.8595840957940644\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "start = time()\n",
    "predictions = lrModel.transform(df_test_tfidf)\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "end = time()\n",
    "print('Evaluation sur le test set prend {}'.format(end-start))\n",
    "print('Accuracy : {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naive Bayes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le training prend 69.61200952529907\n",
      "Evaluation sur le test set prend 99.16759467124939\n",
      "Accuracy : 0.48508329549263274\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "start = time()\n",
    "nb = NaiveBayes(smoothing=1.0)\n",
    "nbModel = nb.fit(df_train_tfidf)\n",
    "end = time()\n",
    "print('Le training prend {}'.format(end-start))\n",
    "predictions = nbModel.transform(df_test_tfidf)\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "end = time()\n",
    "print('Evaluation sur le test set prend {}'.format(end-start))\n",
    "print('Accuracy : {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVM :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le training prend 1731.0260424613953\n",
      "Evaluation sur le test set prend 1755.5469198226929\n",
      "Accuracy : 0.8649745880270016\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "lsvc = LinearSVC(maxIter=10, regParam=0.1)\n",
    "\n",
    "# Fit the model\n",
    "lsvcModel = lsvc.fit(df_train_tfidf)\n",
    "\n",
    "end = time()\n",
    "print('Le training prend {}'.format(end-start))\n",
    "predictions = lsvcModel.transform(df_test_tfidf)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "end = time()\n",
    "print('Evaluation sur le test set prend {}'.format(end-start))\n",
    "print('Accuracy : {}'.format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
